# All <, >, " and # characters that are values for a field contained herein
# must be represented as &lt;, &gt;, &#34; and &#035; respectively.

# When enabled the spider adds NEW pages to your index. 
<newSpideringEnabled>1</>

# When enabled the spider will re-visit and update pages that are already in
# your index.
<oldSpideringEnabled>1</>

# Weight time slices of new spiders in the priority page by this factor
# relative to the old spider queues.
<newSpiderWeight>4.000</>

# If this is enabled others can add web pages to your index via the add url
# page.
<addUrlEnabled>1</>

# Do a tight merge on indexdb and datedb at this time every day. This is
# expressed in MINUTES past midnight UTC. UTC is 5 hours ahead of EST and 7
# hours ahead of MST. Leave this as -1 to NOT perform a daily merge. To merge
# at midnight EST use 60*5=300 and midnight MST use 60*7=420.
<dailyMergeTime>240</>

# Comma separated list of days to merge on. Use 0 for Sunday, 1 for Monday,
# ... 6 for Saturday. Leaving this parmaeter empty or without any numbers will
# make the daily merge happen every day
<dailyMergeDays><![CDATA[0]]></>

# When the daily merge was last kicked off. Expressed in UTC in seconds since
# the epoch.
<dailyMergeLastStarted>1235275216</>

# Index documents for generating results sorted by date or constrained by date
# range. Only documents indexed while this is enabled will be returned for
# date-related searches.
<useDatedb>1</>

# Do not index pubdates into datedb that are more than this many days old. Use
# -1 for no limit. A value of zero essentially turns off datedb. Pre-existing
# pubdates in datedb that fail to meet this constraint WILL BE COMPLETELY
# ERASED when datedb is merged.
<ageCutoffForDatedb>-1</>

# Default timezone to use when none specified on parsed time.  Use offset from
# GMT, i.e 0400 (AMT) or -0700 (MST)
<datedbDefaultTimezone>0</>

# If this is true, users will have to pass a simple Turing test to add a url.
# This prevents automated url submission.
<turingTestEnabled>1</>

# If this is false, the spider will not harvest links from web pages it
# visits. Links that it does harvest will be attempted to be indexed at a
# later time. 
<spiderLinks>1</>

# If this is true the spider will only harvest links to pages that are
# contained on the same host as the page that is being spidered. Example: When
# spidering a page from www.gigablast.com, only links to pages that are from
# www.gigablast.com would be harvested, if this switch were enabled. This
# allows you to seed the spider with URLs from a specific set of hosts and
# ensure that only links to pages that are from those hosts are harvested.
<onlySpiderLinksFromSameHost>0</>

# If less than this many days have elapsed since the last time we added the
# outlinks to spiderdb, do not re-add them to spiderdb. Saves resources.
<doNotReaddOldOutlinksMoreThanThisManyDays>30.000</>

# Perform random searches on googles news search engine to add sites with
# ingoogle tags into tagdb.
<scrapingEnabledWeb>0</>

# Perform random searches on googles news search engine to add sites with news
# and goognews and ingoogle tags into tagdb.
<scrapingEnabledNews>1</>

# Perform random searches on googles news search engine to add sites with
# blogs and googblogs and ingoogle tags into tagdb.
<scrapingEnabledBlogs>1</>

# Add the "sitepathdepth" to tagdb if a hostname is determined to have
# subsites at a particular depth.
<subsiteDetectionEnabled>0</>

# When enabled, the spider will discard web pages which are identical to other
# web pages that are already in the index AND that are from the same hostname.
# An example of a hostname is www1.ibm.com. However, root urls, urls that have
# no path, are never discarded. 
<dedupingEnabled>1</>

# When enabled, the spider will discard web pages which, when a www is
# prepended to the page's url, result in a url already in the index.
<dedupingEnabledForWww>1</>

# Detect and do not index pages which have a 200 status code, but are likely
# to be error pages.
<detectCustomErrorPages>1</>

# Should pages be removed from the index if they are no longer accessible on
# the web?
<delete404s>1</>

# Should documents be deleted from the index if they have been retried them
# enough times and the last received error is a time out? If your internet
# connection is flaky you may say no here to ensure you do not lose important
# docs.
<deleteTimedOutDocs>1</>

# If this is true, the spider, when a url redirects to a "simpler" url, will
# add that simpler url into the spider queue and abandon the spidering of the
# current url.
<useSimplifiedRedirects>1</>

# If this is true, the spider, when updating a web page that is already in the
# index, will not even download the whole page if it hasn't been updated since
# the last time Gigablast spidered it. This is primarily a bandwidth saving
# feature. It relies on the remote webserver's returned Last-Modified-Since
# field being accurate.
<useIfModifiedSince>0</>

# If this is true, the spider, when checking the page if it has changed enough
# to reindex or update the published date, it will build the vector only from
# the content located on that page.
<buildSimilarityVectorFromContentOnly>0</>

# This requires build similarity from content only to be on.  This indexes the
# publish date (only if the content has changed enough) to be between the last
# two spider dates.
<useContentSimilarityToIndexPublishDate>0</>

# This requires build similarity from content only and use content similarity
# to index publish date to be on.  This percentage is the maximum similarity
# that can exist between an old document and new before the publish date will
# be updated.
<maxPercentageSimilarToUpdatePublishDate>60</>

# If this is true Gigablast will respect the robots.txt convention.
<useRobotstxt>1</>

# If this is true and the spider finds lewd words in the hostname of a url it
# will throw that url away. It will also throw away urls that have 5 or more
# hyphens in their hostname.
<doUrlSpornChecking>1</>

# Hours to wait after trying to add an unspiderable url to spiderdb again.
<hoursBeforeAddingUnspiderableUrlToSpiderdb>168</>

# If this is true then new documents will be removed from the index if the
# quota for their domain has been breeched.
<enforceDomainQuotasOnNewDocs>0</>

# If this is true then indexed documents will be removed from the index if the
# quota for their domain has been breeched.
<enforceDomainQuotasOnIndexedDocs>0</>

# Does not use approximations so will do more disk seeks and may impact
# indexing performance significantly.
<useExactQuotas>0</>

# If this is true then only the root indexb file is searched for linkers.
# Saves on disk seeks, but may use older versions of indexed web pages.
<restrictIndexdbForSpidering>0</>

# Do not merge more than this many files during a single merge operation.
# Merge does not scale well to numbers above 50 or so.
<indexdbMaxTotalFilesToMerge>50</>

# Merge is triggered when this many indexdb data files are on disk.
<indexdbMinFilesNeededToTriggerMerge>3</>

# Merge is triggered when this many datedb data files are on disk.
<datedbMinFilesNeededToTriggerToMerge>3</>

# Merge is triggered when this many spiderdb data files are on disk.
<spiderdbMinFilesNeededToTriggerToMerge>3</>

# Merge is triggered when this many checksumdb data files are on disk.
<checksumdbMinFilesNeededToTriggerToMerge>2</>

# Merge is triggered when this many clusterdb data files are on disk.
<clusterdbMinFilesNeededToTriggerToMerge>2</>

# Merge is triggered when this many linkdb data files are on disk.
<linkdbMinFilesNeededToTriggerToMerge>2</>

# Merge is triggered when this many linkdb data files are on disk.
<tagdbMinFilesToMerge>6</>

# Rather than downloading the content again when indexing old urls, use the
# stored content. Useful for reindexing documents under a different ruleset or
# for rebuilding an index. You usually should turn off the 'use robots.txt'
# switch. And turn on the 'use old ips' and 'recycle link votes' switches for
# speed. If rebuilding an index then you should turn off the 'only index
# changes' switches.
<recycleContent>0</>

# If this is true Gigablast will index hyper-link text and use hyper-link
# structures to boost the quality of indexed documents.
<enableLinkVoting>1</>

# If this is true, do not allow spammy inlinks to vote. This check is too
# aggressive for some collections, i.e.  it does not allow pages with cgi in
# their urls to vote.
<doLinkSpamChecking>0</>

# Use the links: termlists instead of link:. Also allows pages linking from
# the same domain or IP to all count as a single link from a different IP.
# This is also required for incorporating RSS and Atom feed information when
# indexing a document.
<useNewLinkAlgo>1</>

# How often should Gigablast recompute the link info for a url. Also applies
# to getting the quality of a site or root url, which is based on the link
# info. In days. Can use decimals. 0 means to update the link info every time
# the url's content is re-indexed. If the content is not reindexed because it
# is unchanged then the link info will not be updated. When getting the link
# info or quality of the root url from an external cluster, Gigablast will
# tell the external cluster to recompute it if its age is this or higher.
<updateLinkInfoFrequency>20.000</>

# If true, we ALWAYS recycle the imported link info and NEVER recompute it
# again. Otherwise, recompute it when we recompute the local link info.
<recycleImportedLinkInfo>1</>

# If this is true Gigablast will only allow one vote per the top 2 significant
# bytes of the IP address. Otherwise, multiple pages from the same top IP can
# contribute to the link text and link-based quality ratings of a particular
# URL. Furthermore, no votes will be accepted from IPs that have the same top
# 2 significant bytes as the IP of the page being indexed.
<restrictLinkVotingByIp>1</>

# If this is true Gigablast will index the plain text surrounding the
# hyper-link text. The score will be x times that of the hyper-link text,
# where x is the scalar below.
<indexInlinkNeighborhoods>1</>

# Sometimes you want the spiders to use the tagdb of another collection, like
# the <i>main</i> collection. If this is empty it defaults to the current
# collection.
<tagdbCollectionName><![CDATA[]]></>

# Spiders will look to see if the current page is in catdb.  If it is, all
# Directory information for that page will be indexed with it.
<catdbLookupsEnabled>1</>

# Rather than requesting new info from DMOZ, like titles and topic ids, grab
# it from old record. Increases performance if you are seeing a lot of
# "getting catdb record" entries in the spider queues.
<recycleCatdbInfo>0</>

# If this is 'NO' then pages that are in catdb, but banned from tagdb or the
# url filters page, can not be banned.
<allowBanningOfPagesInCatdb>1</>

# Ignore and skip spider errors if the spidered site is found in Catdb (DMOZ).
<overrideSpiderErrorsForCatdb>0</>

# If this is disabled the spider will not allow any docs from the gb2312
# charset into the index.
<allowAsianDocs>1</>

# If this is disabled the spider will not allow any docs which contain adult
# content into the index (overides tagdb).
<allowAdultDocs>1</>

# If this is disabled the spider will not allow any xml into the index.
<allowXmlDocs>1</>

# If this is eabled the spider will not allow any docs which are determined to
# be serps.
<doSerpDetection>1</>

# If this is disabled and the proxy IP below is not zero then Gigablast will
# assume all spidered URLs have an IP address of 1.2.3.4.
<doIPLookup>1</>

# Should the stored IP of documents we are reindexing be used? Useful for
# pages banned by IP address and then reindexed with the reindexer tool.
<useOldIPs>0</>

# Remove banned pages from the index. Pages can be banned using tagdb or the
# Url Filters table.
<removeBannedPages>1</>

# If this is false then the filter will not be used on html or text pages.
<applyFilterToTextPages>0</>

# If this is true, spiders will read HTTPS pages using SSL Protocols.
<allowHTTPSPagesUsingSSL>1</>

# If this is YES, Gigablast will attempt to categorize every page as being in
# particular news categories like sports, business, etc. and will be
# searchable by doing a query like "newstopic:sports.
<indexNewsTopics>0</>

# If an item on a page has an RSS feed link, add the RSS link to the spider
# queue and index the RSS pages instead of the current page.
<followRSSLinks>0</>

# Only index pages that were linked to by an RSS feed. Follow RSS Links must
# be enabled (above).
<onlyIndexArticlesFromRSSFeeds>0</>

# Maximum number of urls that can be submitted via the addurl interface, per
# IP domain, per 24 hour period. A value less than or equal to zero implies no
# limit.
<maxAddUrls>100</>

# Gigablast will not download, index or store more than this many bytes of an
# html or text document. Use -1 for no max.
<maxTextDocLength>150000</>

# Gigablast will not download, index or store more than this many bytes of a
# non-html, non-text document. Use -1 for no max.
<maxOtherDocLength>5500000</>

# Program to spawn to filter all HTTP replies the spider receives. Leave blank
# for none.
<filterName><![CDATA[gbfilter]]></>

# Kill filter shell after this many seconds. Assume it stalled permanently.
<filterTimeout>40</>

# Retrieve pages from the proxy at this IP address.
<proxyIp>0.0.0.0</>

# Retrieve pages from the proxy on this port.
<proxyPort>0</>

# How many second to cache a robots.txt file for. 86400 is 1 day. 0 means
# Gigablast will not read from the cache at all and will download the
# robots.txt before every page if robots.txt use is enabled above. However, if
# this is 0 then Gigablast will still store robots.txt files into the cache.
<maxRobotstxtCacheAge>36000</>

# Only spider URLs scheduled to be spidered at this time or after. In UTC.
<spiderStartTime>11 Jan 1970 12:00 UTC</>

# Only spider URLs scheduled to be spidered at this time or before. If "use
# current time" is true then the current local time is used for this value
# instead. in UTC.
<spiderEndTime>21 Feb 2008 12:28 UTC</>

# Use the current time as the spider end time?
<useCurrentTime>1</>

# Should Gigablast compute and store a topics vector for every document
# indexed. This allows Gigablast to do topic clustering without having to
# compute this vector at query time. You can turn topic clustering on in the
# Search Controls page.
<storeTopicsVector>0</>

# If the url's content is over X% similar to what we already have indexed,
# then do not reindex it, and treat the content as if it were unchanged for
# intelligent spider scheduling purposes. Set to 100% to always reindex the
# document, regardless, although the use-ifModifiedSince check above may still
# be in affect, as well as the deduping-enabled check. This will also affect
# the re-spider time, because Gigablast spiders documents that change
# frequently faster.
<maxSimilarityToReindex>90</>

# Distribute web downloads based on the ip of the host so only one spider ip
# hits the same hosting ip.  Helps webmaster's logs look nicer.
<distributeSpiderDownloadBasedOnIp>1</>

# When a spider queue drops below this percent of its max level it will reload
# from disk.
<percentOfWaterMarkToReloadQueues>40</>

# What is the minimum number of days the spider should wait before re-visiting
# a particular web page? The spiders attempts to determine the update cycle of
# each web page and it tries to visit them as needed, but it will not wait
# less than this number of days regardless.
<minRespiderWait>0.200</>

# What is the maximum number of days the spider should wait before re-visiting
# a particular web page?
<maxRespiderWait>90.000</>

# What is the number of days Gigablast should wait before spidering a
# particular web page for the second time? Tag in ruleset will override this
# value if it is present.
<firstRespiderWait>14.000</>

# If a spidered web page has a network error, such as a DNS not found error,
# or a time out error, how many days should Gigablast wait before reattempting
# to spider that web page?
<errorRespiderWait>0.500</>

# If a spidered web page has a http status error, such as a 404 page not found
# error, how many days should Gigablast wait before reattempting to spider
# that web page?
<docNotFoundErrorRespiderWait>7.000</>

# The maximum kilobits per second that the spider can download.
<spiderMaxKbps>-2.000</>

# The maximum number of pages per second that can be indexed or deleted from
# the index.
<spiderMaxPagesPerSecond>-2.000</>

# What is the maximum number of web pages the spider is allowed to download
# simultaneously?
<maxSpiders>30</>

# How many times should the spider be allowed to fail to download a particular
# web page before it gives up? Failure may result from temporary loss of
# internet connectivity on the remote end, dns or routing problems.
<numberRetriesPerUrl>2</>

# Keep this pretty high so that we get problem urls out of the index fast,
# otherwise, you might be waiting months for another retry. Use
# <i>undefined</i> to indicate no change in the priority of the url.
<priorityOfUrlsBeingRetried>1</>

# What is the maximum number of pages that are permitted for this collection?
<maxPagesInIndex>10000000000</>

# Say yes here to make Gigablast import link text from another collection into
# this one when spidering urls. Gigablast will use the hosts.conf file in the
# working directory to tell it what hosts belong to the cluster to import
# from. Gigablast will use the "update link votes frequency" parm above to
# determine if the info should be recomputed on the other cluster.
<importLinkInfo>0</>

# Tell Gigablast to import from the cluster defined by hosts2.conf in the
# working directory, rather than hosts.conf
<useHosts2confForImportCluster>1</>

# Gigablast will fetch the link info from this collection.
<collectionToImportFrom><![CDATA[]]></>

# Tell pageturk to display the tag questions for the comma seperated tag
# names. no space allowed.
<turkTags><![CDATA[blog,spam,news]]></>

# Weight title this much more or less. This units are percentage. A 100 means
# to not give the title any special weight. Generally, though, you want to
# give it significantly more weight than that, so 2400 is the default.
<titleWeight>4600</>

# Weight terms in header tags by this much more or less. This units are
# percentage. A 100 means to not give the header any special weight.
# Generally, though, you want to give it significantly more weight than that,
# so 600 is the default.
<headerWeight>600</>

# Weight text in url path this much more. The units are percentage. A 100
# means to not give any special weight. Generally, though, you want to give it
# significantly more weight than that, so 600 is the default.
<urlPathWordWeight>1600</>

# Weight text in the incoming external link text this much more. The units are
# percentage. It already receives a decent amount of weight naturally.
<externalLinkTextWeight>600</>

# Weight text in the incoming internal link text this much more. The units are
# percentage. It already receives a decent amount of weight naturally.
<internalLinkTextWeight>150</>

# Weight concepts this much more. The units are percentage. It already
# receives a decent amount of weight naturally. AKA: surrounding text boost.
<conceptWeight>100</>

# Boost the score of all terms in the document using this number. The boost
# itself is expressed as a percentage. The boost is 10 * QBB^q, where q is the
# quality of the document, which ranges from 0 to 100, and QBB is this quality
# boost base. The score of each term in the document is multiplied by the
# quality boost. That product becomes the new score of that term.
<qualityBoostBase>1.065</>

# If this is true gigablast will only index the article content on pages
# identifed as permalinks. It will NOT index any page content on non-permalink
# pages, and it will avoid indexing menu content on any page. It will not
# index meta tags on any page. It will only index incoming link text for
# permalink pages. Useful when indexing blog or news sites.
<onlyIndexArticleContent>1</>

# Bytes in tier stage 0.
<tierStage0>528000</>

# Bytes in tier stage 1.
<tierStage1>9000000</>

# Bytes in tier stage 2.
<tierStage2>36800000</>

# Bytes in tier stage 0 raw.
<tierStage0Raw>528000</>

# Bytes in tier stage 1 raw.
<tierStage1Raw>9000000</>

# Bytes in tier stage 2 raw.
<tierStage2Raw>36800000</>

# Bytes in tier stage 0 raw for site: queries.
<tierStage0Sites>528000</>

# Bytes in tier stage 1 raw for site: queries.
<tierStage1Sites>9000000</>

# Bytes in tier stage 2 raw for site: queries.
<tierStage2Sites>36800000</>

# X is the max size in bytes of the compound termlist. Each document id is 6
# bytes.  Should be several times themax tierStage2 size to match large OR
# queries
<compoundListMaxSize>50000000</>

# If this is true Gigablast will exclude documents with dirty words from the
# search results. It will also disallow queries containing dirty words.
# Override with the <i>ff</i> cgi parameter in your query url.
<familyFilterEnabledByDefault>0</>

# If this is true Gigablast will only search the root index file for docIds.
# Saves on disk seeks, but may use older versions of indexed web pages.
<restrictIndexdbForQueries>0</>

# Like above, but specifically for XML feeds.
<restrictIndexdbForXmlFeed>0</>

# Require all terms value for for XML feeds.If the cgi parm "rat" is not
# specified then this parm is considered. A value of 1 is to require all
# terms. A value of 0 means to do a default OR query. A value of 2 means to do
# a default AND query but switch to a default OR query if there are not enough
# search results.
<defaultRequireAllTermsValueForXmlFeed>1</>

# Require all terms value for for HTML feeds.If the cgi parm rat is not
# specified then this parm is considered. A value of 1 is to require all
# terms. A value of 0 means to do a default OR query. A value of 2 means to do
# a default AND query but switch to a default OR query if there are not enough
# search results.
<defaultRequireAllTermsValueForHtmlFeed>1</>

# Should we read search results from the cache? Set to false to fix dmoz bug.
<readFromCacheByDefault>0</>

# Check for successive queries from the same IP. If they do not contain the
# random keyword consider them a robot and deny access.
<doRobotChecking>0</>

# Should search results be site clustered by default?
<siteClusterByDefault>1</>

# Hide all clustered results instead of displaying two results from each site.
<hideAllClusteredResults>0</>

# Should search results be clustered using domain info in IndexTable?
<doInnerLoopSiteClustering>0</>

# Should duplicate search results be removed by default?
<dedupResultsByDefault>1</>

# Used to cluster similar articles in the news collection.
<clusterByTopic>0</>

# Should we dedup URLs with case insensitivity? This is mainly to correct
# duplicate wiki pages.
<dedupURLs>1</>

# Use language specific pages for home, etc.
<useVhostLanguageDetection>1</>

# Use Language weights to sort query results. This will give results of a
# similar language a higher priority.
<useLanguageWeights>1</>

# Default language for post query rerank. This should only be used on limited
# collections. Value should be any language abbreviation, for example "en" for
# English.
<sortLanguagePreference><![CDATA[en_US]]></>

# Default country for post query rerank. This should only be used on limited
# collections. Value should be any country code abbreviation, for example "us"
# for United States.
<sortCountryPreference><![CDATA[us]]></>

# Language method weights for spider language detection. A string of ascii
# numerals that should default to 895768712
<languageMethodWeights><![CDATA[894768712]]></>

# Language detection sensitivity. Higher values mean higher hitrate, but lower
# accuracy. Suggested values are from 2 to 20
<languageDetectionSensitivity>5</>

# Language detection threshold sensitivity. Higher values mean better
# accuracy, but lower hitrate. Suggested values are from 2 to 20
<languageDetectionThreshold>3</>

# Language detection size. Higher values mean more accuracy, but longer
# processing time. Suggested values are 300-1000
<languageDetectionSamplesize>600</>

# Language detection page sample size. Higher values mean more accuracy, but
# longer spider time. Suggested values are 3000-10000
<languageDetectionSpiderSamplesize>6000</>

# How many search results should we scan for post query demotion? 0 disables
# all post query reranking. 
<docsToCheckForPostQueryDemotion>99</>

# Demotion factor of non-relevant languages.  Score will be penalized by this
# factor as a percent if it's language is foreign. A safe value is probably
# anywhere from 0.5 to 1. 
<demotionForForeignLanguages>0.999</>

# Demotion factor for unknown languages. Page's score will be penalized by
# this factor as a percent if it's language is not known. A safe value is 0,
# as these pages will be reranked by country (see below). 0 means no demotion.
<demotionForUnknownLanguages>0.000</>

# Demotion for pages where the country of the page writes in the same language
# as the country of the query. If query language is the same as the language
# of the page, then if a language written in the country of the page matches a
# language written by the country of the query, then page's score will be
# demoted by this factor as a percent. A safe range is between 0.5 and 1. 
<demotionForPagesWhereTheCountryOfThePageWritesInTheSameLanguageAsTheCountryOfTheQuery>0.980</>

# Demotion factor for query terms or gigabits in a result's url. Score will be
# penalized by this factor times the number of query terms or gigabits in the
# url divided by the max value below such that fewer query terms or gigabits
# in the url causes the result to be demoted more heavily, depending on the
# factor. Higher factors demote more per query term or gigabit in the page's
# url. Generally, a page may not be demoted more than this factor as a
# percent. Also, how it is demoted is dependant on the max value. For example,
# a factor of 0.2 will demote the page 20% if it has no query terms or
# gigabits in its url. And if the max value is 10, then a page with 5 query
# terms or gigabits in its url will be demoted 10%; and 10 or more query terms
# or gigabits in the url will not be demoted at all. 0 means no demotion. A
# safe range is from 0 to 0.35. 
<demotionForQueryTermsOrGigabitsInUrl>0.000</>

# Max number of query terms or gigabits in a url. Pages with a number of query
# terms or gigabits in their urls greater than or equal to this value will not
# be demoted. This controls the range of values expected to represent the
# number of query terms or gigabits in a url. It should be set to or near the
# estimated max number of query terms or topics that can be in a url. Setting
# to a lower value increases the penalty per query term or gigabit that is not
# in a url, but decreases the range of values that will be demoted.
<maxValueForPagesWithQueryTermsOrGigabitsInUrl>10</>

# Demotion factor for pages that are not high quality. Score is penalized by
# this number as a percent times level of quality. A pqge will be demoted by
# the formula (max quality - page's quality) * this factor / the max value
# given below. Generally, a page will not be demoted more than this factor as
# a percent. 0 means no demotion. A safe range is between 0 to 1. 
<demotionForPagesThatAreNotHighQuality>0.000</>

# Max page quality. Pages with a quality level equal to or higher than this
# value will not be demoted. 
<maxValueForPagesThatAreNotHighQuality>100</>

# Demotion factor each path in the url. Score will be demoted by this factor
# as a percent multiplied by the number of paths in the url divided by the max
# value below. Generally, the page will not be demoted more than this value as
# a percent. 0 means no demotion. A safe range is from 0 to 0.75. 
<demotionForPagesThatAreNotRootOrHaveManyPathsInTheUrl>0.000</>

# Max number of paths in a url. This should be set to a value representing a
# very high number of paths for a url. Lower values increase the difference
# between how much each additional path demotes. 
<maxValueForPagesThatHaveManyPathsInTheUrl>16</>

# Demotion factor for pages that do not have a catid. Score will be penalized
# by this factor as a percent. A safe range is from 0 to 0.2. 
<demotionForPagesThatDoNotHaveACatid>0.000</>

# Demotion factor for pages where smallest catid has a lot of super topics.
# Page will be penalized by the number of super topics multiplied by this
# factor divided by the max value given below. Generally, the page will not be
# demoted more than this factor as a percent. Note: pages with no catid are
# demoted by this factor as a percent so as not to penalize pages with a
# catid. 0 means no demotion. A safe range is between 0 and 0.25. 
<demotionForPagesWhereSmallestCatidHasALotOfSuperTopics>0.000</>

# Max number of super topics. Pages whose smallest catid that has more super
# topics than this will be demoted by the maximum amount given by the factor
# above as a percent. This should be set to a value representing a very high
# number of super topics for a category id. Lower values increase the
# difference between how much each additional path demotes. 
<maxValueForPagesWhereSmallestCatidHasALotOfSuperTopics>16</>

# Demotion factor for larger pages. Page will be penalized by its size times
# this factor divided by the max page size below. Generally, a page will not
# be demoted more than this factor as a percent. 0 means no demotion. A safe
# range is between 0 and 0.25. 
<demotionForLargerPages>0.000</>

# Max page size. Pages with a size greater than or equal to this will be
# demoted by the max amount (the factor above as a percent). 
<maxValueForLargerPages>65535</>

# Demotion factor for non-location specific queries with a location specific
# title. Pages which contain a location in their title which is not in the
# query or the gigabits will be demoted by their population multiplied by this
# factor divided by the max place population specified below. Generally, a
# page will not be demoted more than this value as a percent. 0 means no
# demotion. 
<demotionForNonlocationSpecificQueriesWithALocationSpecificTitle>0.000</>

# Demotion factor for non-location specific queries with a location specific
# summary. Pages which contain a location in their summary which is not in the
# query or the gigabits will be demoted by their population multiplied by this
# factor divided by the max place population specified below. Generally, a
# page will not be demoted more than this value as a percent. 0 means no
# demotion. 
<demotionForNonlocationSpecificQueriesWithALocationSpecificSummary>0.000</>

# Demotion factor for non-location specific queries with a location specific
# dmoz regional category. Pages which contain a location in their dmoz which
# is not in the query or the gigabits will be demoted by their population
# multiplied by this factor divided by the max place population specified
# below. Generally, a page will not be demoted more than this value as a
# percent. 0 means no demotion. 
<demotionForNonlocationSpecificQueriesWithALocationSpecificDmozCategory>0.000</>

# Demote locations that appear in gigabits.
<demoteLocationsThatAppearInGigabits>1</>

# Max place population. Places with a population greater than or equal to this
# will be demoted to the maximum amount given by the factor above as a
# percent. 
<maxValueForNonlocationSpecificQueriesWithLocationSpecificResults>100000</>

# Demotion factor for content type that is non-html. Pages which do not have
# an html content type will be demoted by this factor as a percent. 0 means no
# demotion. A safe range is between 0 and 0.35. 
<demotionForNonhtml>0.000</>

# Demotion factor for content type that is xml. Pages which have an xml
# content type will be demoted by this factor as a percent. 0 means no
# demotion. Any value between 0 and 1 is safe if demotion for non-html is set
# to 0. Otherwise, 0 should probably be used. 
<demotionForXml>0.950</>

# Demotion factor for pages with fewer other pages from same hostname. Pages
# with results from the same host will be demoted by this factor times each
# fewer host than the max value given below, divided by the max value.
# Generally, a page will not be demoted more than this factor as a percent. 0
# means no demotion. A safe range is between 0 and 0.35. 
<demotionForPagesWithOtherPagesFromSameHostname>0.000</>

# Max number of pages from same domain. Pages which have this many or more
# pages from the same domain will not be demoted. 
<maxValueForPagesWithOtherPagesFromSameDomain>12</>

# Initial demotion factor for pages with common topics in dmoz as other
# results. Pages will be penalized by the number of common topics in dmoz
# times this factor divided by the max value given below. Generally, a page
# will not be demoted by more than this factor as a percent. Note: this factor
# is decayed by the factor specified in the parm below, decay for pages with
# common topics in dmoz as other results, as the number of pages with common
# topics in dmoz increases. 0 means no demotion. A safe range is between 0 and
# 0.35. 
<initialDemotionForPagesWithCommonTopicsInDmozAsOtherResults>0.000</>

# Decay factor for pages with common topics in dmoz as other results. The
# initial demotion factor will be decayed by this factor as a percent as the
# number of common topics increase. 0 means no decay. A safe range is between
# 0 and 0.25. 
<decayForPagesWithCommonTopicsInDmozAsOtherResults>0.150</>

# Max number of common topics in dmoz as other results. Pages with a number of
# common topics equal to or greater than this value will be demoted to the
# maximum as given by the initial factor above as a percent. 
<maxValueForPagesWithCommonTopicsInDmozAsOtherResults>32</>

# Demotion factor for pages where dmoz category names contain fewer query
# terms or their synonyms. Pages will be penalized for each query term or
# synonym of a query term less than the max value given below multiplied by
# this factor, divided by the max value. Generally, a page will not be demoted
# more than this value as a percent. 0 means no demotion. A safe range is
# between 0 and 0.3. 
<demotionForPagesWhereDmozCategoryNamesContainQueryTermsOrTheirSynonyms>0.000</>

# Max number of query terms and their synonyms in a page's dmoz category name.
# Pages with a number of query terms or their synonyms in all dmoz category
# names greater than or equal to this value will not be demoted. 
<maxValueForPagesWhereDmozCategoryNamesContainQueryTermsOrTheirSynonyms>10</>

# Demotion factor for pages where dmoz category names contain fewer gigabits.
# Pages will be penalized by the number of gigabits in all dmoz category names
# fewer than the max value given below divided by the max value. Generally, a
# page will not be demoted more than than this factor as a percent. 0 means no
# demotion. A safe range is between 0 and 0.3. 
<demotionForPagesWhereDmozCategoryNamesContainGigabits>0.000</>

# Max number of pages where dmoz category names contain a gigabit. Pages with
# a number of gigabits in all dmoz category names greater than or equal to
# this value will not be demoted. 
<maxValueForPagesWhereDmozCategoryNamesContainGigabits>16</>

# Demotion factor for pages based on datedb date. Pages will be penalized for
# being published earlier than the max date given below. The older the page,
# the more it will be penalized based on the time difference between the
# page's date and the max date, divided by the max date. Generally, a page
# will not be demoted more than this value as a percent. 0 means no demotion.
# A safe range is between 0 and 0.4. 
<demotionForPagesBasedOnDatedbDate>0.000</>

# Pages with a publish date equal to or earlier than this date will be demoted
# to the max (the factor above as a percent). Use this parm in conjunction
# with the max value below to specify the range of dates where demotion
# occurs. If you set this parm near the estimated earliest publish date that
# occurs somewhat frequently, this method can better control the additional
# demotion per publish day. This number is given as seconds since the epoch,
# January 1st, 1970 divided by 1000. 0 means use the epoch. 
<minValueForDemotionBasedOnDatedbDate>631177</>

# Pages with a publish date greater than or equal to this value divided by
# 1000 will not be demoted. Use this parm in conjunction with the min value
# above to specify the range of dates where demotion occurs. This number is
# given as seconds before the current date and time taken from the system
# clock divided by 1000. 0 means use the current time of the current day. 
<maxValueForDemotionBasedOnDatedbDate>0</>

# Demotion factor for proximity of query terms in a document.  The closer
# together terms occur in a document, the higher it will score.0 means no
# demotion. 
<demotionForPagesBasedOnProximity>0.900</>

# Demotion factor for where the query terms occur in the document.  If the
# terms only occur in a menu, a link, or a list, the document will be
# punished.0 means no demotion. 
<demotionForPagesBasedOnQueryTermsSection>0.900</>

# The proportion that the original score affects its rerank position. A factor
# of 1 will maintain the original score, 0 will only use the indexed score to
# break ties.
<weightOfIndexedScoreOnPqr>0.500</>

# Max summary score where no more demotion occurs above. Pages with a summary
# score greater than or equal to this value will not be demoted. 
<maxValueForDemotionForPagesBasedOnProximity>100000</>

# Search result which contains the query terms only as a subphrase of a larger
# phrase will have its score  reduced by this percent.
<demotionForQueryBeingExclusivlyInASubphrase>0.999</>

# Based on the number of inlinks a search results has which are in common with
# another search result.
<demotionBasedOnCommonInlinks>0.500</>

# Do tier jumping. Jumps to the last tier if the number of terms in the query
# are more than 6
<doTierJumping>0</>

# Allows more results to be gathered in the case of an index having a high
# rate of duplicate results.  Generally expressed as 1.2
<numberOfDocumentCallsMultiplier>1.300</>

# Limit number of documents to search that do not provide the required results.
<maxDocumentsToComputePerHost>10001</>

# Limit number of linksdb inlinks requested per result.
<maxRealTimeInlinks>10000</>

# If document summary is this percent similar to a document summary above it,
# then remove it from the search results. 100 means only to remove if exactly
# the same. 0 means no summary deduping.
<percentSimilarDedupSummary>70</>

# Sets the number of lines to generate for summary deduping. This is to help
# the deduping process not thorw out valid summaries when normally displayed
# summaries are smaller values. Requires percent similar dedup summary to be
# enabled.
<numberOfLinesToUseInSummaryToDedup>3</>

# Show classification options on search results page. Allows the user to pick
# categories for a page.
<showClassificationOptionsOnSearchResultsPage>0</>

# Like above, but used for deciding when to cluster results by topic for the
# news collection.
<percentTopicSimilarDefault>50</>

# Do not allow more than this many query terms. Will return error in XML feed
# error tag if breeched.
<maxQueryTerms>30</>

# Where do we send requests for definitions of search terms. Set to the empty
# string to turn this feature off.
<dictionarySite><![CDATA[http://www.answers.com/]]></>

# Should Gigablast only get one document per IP domain and per domain for
# topic generation?
<ipRestrictionForTopics>1</>

# Should Gigablast remove overlapping topics?
<removeOverlappingTopics>1</>

# How many search results should we scan for related topics per query?
<docsToScanForTopics>30</>

# What is the number of related topics displayed per query? Set to 0 to save
# CPU time.
<numberOfRelatedTopics>11</>

# Related topics with scores below this will be excluded. Scores range from 0%
# to over 100%.
<minTopicsScore>5</>

# How many documents must contain the topic for it to be displayed.
<minTopicDocCount>2</>

# If a document is this percent similar to another document with a higher
# score, then it will not contribute to the topic generation.
<dedupDocPercentForTopics>80</>

# Maximum number of words a topic can have. Affects raw feeds, too.
<maxWordsPerTopic>5</>

# Max chars to sample from each doc for topics.
<topicMaxSampleSize>1024</>

# Max sequential punct chars allowed in a topic. Set to 1 for speed, 5 or more
# for best topics but twice as slow.
<topicMaxPunctLen>1</>

# If enabled while using the XML feed, when Gigablast finds a spelling
# recommendation it will be included in the XML <spell> tag. Default is 0 if
# using an XML feed, 1 otherwise.
<doSpellChecking>0</>

# display narrow search results. 
<displayNarrowSearch>0</>

# Allows anyone access to perform links: searches on this collection.
<allowLinksSearches>0</>

# What is the number of reference pages to generate per query? Set to 0 to
# save CPU time.
<numberOfReferencePagesToGenerate>30</>

# What is the number of reference pages to display per query?
<numberOfReferencePagesToDisplay>3</>

# How many search results should we scan for reference pages per query?
<docsToScanForReferencePages>0</>

# References with page quality below this will be excluded.  (set to 101 to
# disable references while still generating related pages.
<minReferencesQuality>1</>

# References need this many links to results to be included.
<minLinksPerReferences>2</>

# Stop processing referencing pages after hitting this limit.
<maxLinkersToConsiderForReferencesPerPage>500</>

# Use this multiplier to fetch more than the required number of reference
# pages.  fetches N * (this parm) references and displays the top scoring N.
<pageFetchMultiplierForReferences>1.500</>

# A in A * numLinks + B * quality + C * numLinks/totalLinks.
<numberOfLinksCoefficient>0</>

# B in A * numLinks + B * quality + C * numLinks/totalLinks.
<qualityCoefficient>1</>

# C in A * numLinks + B * quality + C * numLinks/totalLinks.
<linkDensityCoefficient>10</>

# [+|*] in A * numLinks + B * quality [+|*] C * numLinks/totalLinks.
<addOrMultipyQualityTimesLinkDensity>1</>

# maximum allowed value for numReferences parameter
<maximumAllowedValueForNumReferencesParameter>100</>

# maximum allowed value for docsToScanForReferences parameter
<maximumAllowedValueForDocsToScanForReferencesParameter>100</>

# maximum allowed value for maxLinkers parameter
<maximumAllowedValueForMaxLinkersParameter>5000</>

# maximum allowed value for additionalTRFetch parameter
<maximumAllowedValueForAdditionalTRFetch>10.000</>

# number of related pages to generate.
<numberOfRelatedPagesToGenerate>0</>

# number of related pages to display.
<numberOfRelatedPagesToDisplay>0</>

# number of links per reference page to scan for related pages.
<numberOfLinksToScanForRelatedPages>1024</>

# related pages with a quality lower than this will be ignored.
<minRelatedPageQuality>30</>

# related pages with an adjusted score lower than this will be ignored.
<minRelatedPageScore>1</>

# related pages with less than this number of links will be ignored.
<minRelatedPageLinks>2</>

# A in A * numLinks + B * avgLnkrQlty + C * PgQlty + D * numSRPLinks.
<coefficientForNumberOfLinksInRelatedPagesScoreCalculation>10</>

# B in A * numLinks + B * avgLnkrQlty + C * PgQlty + D * numSRPLinks.
<coefficientForAverageLinkerQualityInRelatedPagesScoreCalculation>1</>

# C in A * numLinks + B * avgLnkrQlty + C * PgQlty + D * numSRPLinks
<coefficientForPageQualityInRelatedPagesScoreCalculation>1</>

# D in A * numLinks + B * avgLnkrQlty + C * PgQlty + D * numSRPLinks.
<coefficientForSearchResultLinksInRelatedPagesScoreCalculation>1</>

# What is the maximum number of excerpts displayed in the summary of a related
# page?
<numberOfRelatedPageSummaryExcerpts>1</>

# Highlight query terms in related pages summary.
<highlightQueryTermsInRelatedPagesSummary>1</>

# Truncates a related page title after this many charaters and adds ...
<numberOfCharactersToDisplayInTitleBeforeTruncating>70</>

# Use the search results' links in order to generate related pages.
<useResultsPagesAsReferences>1</>

# Say yes here to make Gigablast check another Gigablast cluster for title rec
# for related pages. Gigablast will use the hosts2.conf file in the working
# directory to tell it what hosts belong to the other cluster.
<getRelatedPagesFromOtherCluster>0</>

# Gigablast will fetch the related pages title record from this collection in
# the other cluster.
<collectionForOtherRelatedPagesCluster><![CDATA[main]]></>

# maximum allowed value for numToGenerate parameter
<maximumAllowedValueForNumToGenerateParameter>100</>

# maximum allowed value for numRPLinksPerDoc parameter
<maximumAllowedValueForNumRPLinksPerDocParameter>5000</>

# maximum allowed value for numSummaryLines parameter
<maximumAllowedValueForNumSummaryLinesParameter>10</>

# Gigablast will import X search results from the external cluster given by
# hosts2.conf and merge those search results into the current set of search
# results. Set to 0 to disable.
<howManyImportedResultsShouldWeInsert>0</>

# The score of all imported results will be multiplied by this number. Since
# results are mostly imported from a large collection they will usually have
# higher scores because of having more link texts or whatever, so tone it down
# a bit to put it on par with the integrating collection.
<importedScoreWeight>0.800</>

# The urls of imported search results must be linked to by at least this many
# documents in the primary collection.
<howManyLinkersMustEachImportedResultHave>3</>

# The number of linkers an imported result has from the base collection is
# multiplied by this weight and then added to the final score. The higher this
# is the more an imported result with a lot of linkers will be boosted.
# Currently, 100 is the max number of linkers permitted.
<numLinkersWeight>50</>

# Gigablast will import X search results from this external collection and
# merge them into the current search results.
<theNameOfTheCollectionToImportFrom><![CDATA[main]]></>

# What is the limit to the total number of returned search results.
<maxSearchResults>1000</>

# What is the limit to the total number of returned search results per query?
<maxSearchResultsPerQuery>1000</>

# What is the limit to the total number of returned search results for clients.
<maxSearchResultsForPayingClients>1000</>

# What is the limit to the total number of returned search results per query
# for paying clients? Auto ban must be enabled for this to work.
<maxSearchResultsPerQueryForPayingClients>100000</>

# Max similar results to show when clustering by topic.
<maxSimilarResultsForClusterByTopic>10</>

# number of extra results to get for cluster by topic
<numberOfExtraResultsToGetForClusterByTopic>100</>

# What is the maximum number of characters allowed in titles displayed in the
# search results?
<maxTitleLen>80</>

# Minimum number of in linkers required to consider getting	the title from in
# linkers
<MinimumNumberOfInLinkersRequiredToConsiderGettingTheTitleFromInLinkers>10</>

# Max number of in linkers to consider for getting in linkers titles.
<MaxNumberOfInLinkersToConsider>80</>

# Also used for gigabits and titles.
<useNewSummaryGenerator>1</>

# 0 = old compatibility mode, 1 = UTF-8 mode, 2 = fast ASCII mode, 3 = Ascii
# Proximity Summary, 4 = Utf8 Proximity Summary, 5 = Ascii Pre Proximity
# Summary, 6 = Utf8 Pre Proximity Summary:
<summaryMode>0</>

# What is the maximum number of characters displayed in a summary for a search
# result?
<maxSummaryLen>2048</>

# What is the maximum number of excerpts displayed in the summary of a search
# result?
<maxSummaryExcerpts>10</>

# What is the maximum number of characters allowed per summary excerpt?
<maxSummaryExcerptLength>255</>

# What is the default number of summary excerpts displayed per search result?
<defaultNumberOfSummaryExcerpts>1</>

# <br> tags are inserted to keep the number of chars in the summary per line
# at or below this width. Strings without spaces that exceed this width are
# not split.
<maxSummaryLineWidth>83</>

# Maximum number of characters to allow in between search terms.
<ProxSummaryCarverRadius>256</>

# Front html tag used for highlightig query terms in the summaries displated
# in the search results.
<frontHighlightTag><![CDATA[&lt;b style=&#34;color:black;background-color:&#035;ffff66&#34;&gt;]]></>

# Front html tag used for highlightig query terms in the summaries displated
# in the search results.
<backHighlightTag><![CDATA[&lt;/b&gt;]]></>

# If enabled, search results shall feed the page turk is used to mechanically
# rank websites.
<enablePageTurk>1</>

# Query expansion will include word stems and synonyms in its search results.
<doQueryExpansion>0</>

# Can Gigablast make titles from the document content? Used mostly for the
# news collection where the title tags are not very reliable.
<considerTitlesFromBody>0</>

# If enabled, results in dmoz will display their categories on the results
# page.
<displayDmozCategoriesInResults>1</>

# If enabled, results in dmoz will display their indirect categories on the
# results page.
<displayIndirectDmozCategoriesInResults>1</>

# If enabled, a link will appear next to each category on each result allowing
# the user to perform their query on that entire category.
<displaySearchCategoryLinkToQueryCategoryOfResult>1</>

# Yes to use DMOZ given title when a page is untitled but is in DMOZ.
<useDmozForUntitled>1</>

# Yes to always show DMOZ summaries with search results that are in DMOZ.
<showDmozSummaries>1</>

# Yes to display the Adult category in the Top category
<showAdultCategoryOnTop>0</>

# If enabled, we show certain tagb tags for each search result, allow
# &amp;inlinks=1 cgi parms, show <docsInColl>, etc. in the xml feed. Created
# for buzzlogic.
<showSensitiveInfoInXmlFeed>1</>

# Display the indexed date along with results.
<displayIndexedDate>1</>

# Display the last modified date along with results.
<displayLastModifiedDate>0</>

# Display the published (datedb) date along with results.
<displayPublishedDate>1</>

# The [cached] link on results pages loads click n scroll.
<enableClicknScroll>0</>

# Enable/disable the use of a remote account verification for Data Feed
# Customers.
<useDataFeedAccountServer>0</>

# The ip address of the Gigablast data feed server to retrieve customer
# account information from.
<dataFeedServerIp>127.0.0.1</>

# The port of the Gigablast data feed server to retrieve customer account
# information from.
<dataFeedServerPort>8040</>

# The collection on the Gigablast data feed server to retrieve customer
# account information from.
<dataFeedServerCollection><![CDATA[customers]]></>

# How many columns results should be shown in. (1-6)
<NumberOfColumns16>1</>

# screen size of browser window
<ScreenWidth>1100</>

# Hostname that will default to this collection. Blank for none or default
# collection.
<collectionHostname><![CDATA[]]></>

# Hostname that will default to this collection. Blank for none or default
# collection.
<collectionHostname1><![CDATA[]]></>

# Hostname that will default to this collection. Blank for none or default
# collection.
<collectionHostname2><![CDATA[]]></>

# Html to display before the search results. Convenient for changing colors
# and displaying logos. Use the variable, %q, to represent the query to
# display in a text box. Use %e to display it in a url.  Use %e to print the
# page encoding.Use %D to print a drop down menu for the number of search
# results to return. Use %S to print sort by date or relevance link. Use %L to
# display the logo. Use %R to display radio buttons for site search. Use %F to
# begin the form. and use %H to insert hidden text boxes of parameters, both
# %F and %H are necessary. Use %f to display the family filter radio buttons.
# Directory: Use %s to display the directory search type options. Use %l to
# specify the location of dir=rtl in the body tag for RTL pages. IMPORTANT: In
# the xml configuration file, this html must be encoded (less thans mapped to
# &lt;, etc.).
<htmlHead><![CDATA[&lt;!DOCTYPE html PUBLIC &#34;-//W3C//DTD HTML 4.01 Transitional//EN&#34;&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Gigablast Search Results: %q&lt;/title&gt;
&lt;style&gt;&lt;!--
body {
font-family:Arial, Helvetica, sans-serif;
color: &#035;000000;
font-size: 12px;
margin: 20px 5px;
letter-spacing: 0.04em;
}
a:link {color:&#035;00c}
a:visited {color:&#035;551a8b}
a:active {color:&#035;f00}
.bold {font-weight: bold;}
.bluetable {background:&#035;d1e1ff;margin-bottom:15px;font-size:12px;}
.url {color:&#035;008000;}
.cached, .cached a {font-size: 10px;color: &#035;666666;
}
table {
font-family:Arial, Helvetica, sans-serif;
color: &#035;000000;
font-size: 12px;
}
.directory {font-size: 16px;}
--&gt;
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;table&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href=/&gt;&lt;img border=0 src=logo.gif&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;
%F
%H
&lt;input name=q type=text size=60 value=&#34;%q&#34;&gt;
&lt;input type=submit value=&#34;Blast Off!&#34;&gt;
&lt;/form&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
]]></>

# Html to display after the search results.
<htmlTail><![CDATA[&lt;br&gt;
&lt;center&gt;
Try your search on 
&lt;a href=http://www.gigablast.com/search?q=%e&gt;gigablast&lt;/a&gt; &amp;nbsp; 
&lt;a href=http://www.google.com/search?q=%e&gt;google&lt;/a&gt; &amp;nbsp;
&lt;a href=http://search.yahoo.com/bin/search?p=%e&gt;yahoo&lt;/a&gt; &amp;nbsp;
&lt;a href=http://search.msn.com/results.aspx?q=%e&gt;msn&lt;/a&gt; &amp;nbsp; 
&lt;a href=http://s.teoma.com/search?q=%e&gt;ask&lt;/a&gt; &amp;nbsp;
&lt;a href=http://search.dmoz.org/cgi-bin/search?search=%e&gt;dmoz&lt;/a&gt; &amp;nbsp;
&lt;/center&gt;
&lt;/body&gt;
&lt;/html&gt;
]]></>

# Html to display for the home page. Use %N for total number of pages indexed.
# Use %n for number of pages indexed for the current collection. Use %H so
# Gigablast knows where to insert the hidden form input tags, which must be
# there. Use %T to display the standard footer and %q to display the query in
# a text box. Use %t to display the directory TOP.
<homePage><![CDATA[&lt;html&gt;
&lt;head&gt;
&lt;meta http-equiv=&#34;Content-Type&#34; content=&#34;text/html; charset=iso-8859-1&#34;&gt;&lt;meta name=&#34;description&#34; content=&#34;A powerful, new search engine that does real-time indexing.&#34;&gt;
&lt;meta name=&#34;keywords&#34; content=&#34;search, search engine, search engines, search the web, fresh index&#34;&gt;
&lt;title&gt;Gigablast&lt;/title&gt;
&lt;style&gt;&lt;!--
body {
font-family:Arial, Helvetica, sans-serif;
color: &#035;000000;
font-size: 12px;
margin: 20px 5px;
letter-spacing: 0.04em;
}
a:link {color:&#035;00c}
a:visited {color:&#035;551a8b}
a:active {color:&#035;f00}
.bold {font-weight: bold;}
.bluetable {background:&#035;d1e1ff;margin-bottom:15px;font-size:12px;}
.url {color:&#035;008000;}
.cached, .cached a {font-size: 10px;color: &#035;666666;
}
table {
font-family:Arial, Helvetica, sans-serif;
color: &#035;000000;
font-size: 12px;
}
.directory {font-size: 16px;}
--&gt;
&lt;/style&gt;
&lt;/head&gt;
&lt;script&gt;
&lt;!--
function x(){document.f.q.focus();}
// --&gt;&lt;/script&gt;
&lt;body onload=&#34;x()&#34;&gt;
&lt;br&gt;&lt;br&gt;
&lt;center&gt;&lt;a href=/&gt;&lt;img border=0 src=logo.gif&gt;&lt;/a&gt;
&lt;br&gt;&lt;br&gt;
&lt;b&gt;Test Cluster&lt;/b&gt;
&lt;br&gt;&lt;br&gt;
&lt;br&gt;&lt;br&gt;
&lt;form method=get action=/search name=f&gt;%H
&lt;input type=hidden name=n value=20&gt;
&lt;input name=q type=text size=60 value=&#34;%q&#34;&gt;&amp;nbsp;&lt;input type=&#34;submit&#34; value=&#34;Blast Off!&#34;&gt;
&lt;br&gt;&lt;br&gt;
Age within:
&lt;input type=radio name=secs value=3600&gt;1 hour
&lt;input type=radio name=secs value=43200&gt;12 hours
&lt;input type=radio name=secs value=86400&gt;1 day
&lt;input type=radio name=secs value=172800&gt;2 days
&lt;input type=radio name=secs value=604800&gt;a week
&lt;input type=radio name=secs value=18748800&gt;a month
&lt;input type=radio name=secs value=220752000&gt;a year
&lt;input type=radio name=secs value=&gt;no restriction
&lt;/form&gt;
&lt;br&gt;
&lt;font size=-1&gt;&lt;a href=adv.html&gt;Advanced Search&lt;/a&gt;&lt;/font&gt;
&lt;br&gt;&lt;br&gt;
&lt;b&gt;%N pages indexed&lt;/b&gt;
&lt;br&gt;&lt;br&gt;
 &lt;a href=/login&gt;Login&lt;/a&gt; | &lt;a href=/pageturkhome?c=main&gt;Turk Test&lt;/a&gt;
&lt;br&gt;&lt;br&gt;
&lt;table&gt;&lt;tr&gt;&lt;td&gt;%T&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;/center&gt;&lt;/body&gt;&lt;/html&gt;]]></>

# Passwords allowed to perform searches on this collection. If no passwords
# are specified, then anyone can search it.
# Use <searchPassword> tag.

# These IPs are not allowed to search this collection or use add url. Useful
# to keep out miscreants. Use zero for the last number of the IP to ban an
# entire IP domain.
<bannedIp>69.93.79.178</>
<bannedIp>66.186.8.50</>

# Enable quality agent on all hosts for this collection
<allAgentsOn>0</>

# Disable quality agent on all hosts for this collection
<allAgentsOff>0</>

# If enabled, the agent will find quality modifiers for all of the sites found
# in titledb.
<qualityAgentEnabled>0</>

# If enabled, the agent will loop when it reaches the end of titledb.
# Otherwise, it will disable itself.
<qualityAgentContinuousLoop>0</>

# If enabled, the agent will look at the paths of its titlerec sample, if the
# offending spam scores all come from the same subsite, we just ban that one. 
# Good for banning hijacked forums or spammed archives.
<banSubsites>0</>

# The agent will start at this docid when scanning titledb looking for sites.
<startDocument>0</>

# The quality agent will try to reexamine entries in tagdb which were added
# more than this many seconds ago
<siteQualityRefreshRate>259200000</>

# Lookup the qualities of this many links in tagdb.
<linkSamplesToGet>256</>

# The quality agent will skip this site if there are less than this many pages
# to evaluate.
<minPagesToEvaluate>1</>

# Decrease a page's spam score if it has a high link quality.  The bonus is
# computed by dividing the page's link quality by this parm.  LinkInfos older
# than 30 days are considered stale and are not used.
<linkBonusDivisor>20</>

# Subtract x points per banned site that a site links to.
<pointsPerBannedLink>3</>

# Subtract x points per site linked to that is on the same IP as other links. 
# Good for catching domain parking lots and spammers in general, but looking
# up the IPs slows down the agent considerably.  (set to 0 to disable.)
<pointsPerLinkToDifferentSitesOnTheSameIP>0</>

# Examine this many sites on the same ip as this site
<numberOfSitesOnAnIpToSample>50</>

# Subtract x points from a site quality for each banned site on the ip
<pointsPerBannedSiteOnIp>2</>

# The penalty for being on a bad IP will not exceed this value.
<maxPenaltyFromBeingOnABadIP>-30</>

# The agent will not process more than this many sites per second.  Can be
# less than 1.
<maxSitesPerSecond>99999.000</>

# Site agent will assign this ruleset to documents  which are determined to be
# low quality.
<siteAgentBannedRuleset>30</>

# If the site has a spam score greater than this parm, it will be inserted
# into the above ruleset.
<banQualityTheshold>-100</>

# If the site has a quality less than this parm, it will be added to the
# spider queue for reindexing
<thesholdToTriggerSiteReindex>-100</>

# The number of ads we would like returned from the ad server. This applies to
# all paid inclusion ads below.
<numAdsInPaidInclusionAdFeed>2</>

# The number of ads we would like returned from the ad server. This applies to
# all skyscraper ads below.
<numAdsInSkyscraperAdFeed>5</>

# The width of the skyscraper ad column in pixels
<skyscraperAdWidth>300</>

# The time (in milliseconds) to wait for an ad list to be returned before
# timing out and displaying the results without any ads. This applies to all
# ads below.
<adFeedTimeout>1000</>

# Enable/Disable the paid inclusion ad.
<1PaidInclusionAdEnable>1</>

# Full link with address and parameters to retrieve an ad feed.  To specify
# parameter input: %q for query, %n for num results, %p for page number, %i
# for query ip, and %% for %.
<1PaidInclusionAdFeedLink><![CDATA[http://www.searchfeed.com/rd/feed/XMLFeed.jsp?trackId=O7850680825&amp;pID=68925&amp;cat=%q&amp;nl=%n&amp;page=%p&amp;ip=%i]]></>

# Specify the full xml path for a result.
<1PaidInclusionAdFeedXmlResultTag><![CDATA[listings.listing]]></>

# Specify the full xml path for the results title.
<1PaidInclusionAdFeedXmlTitleTag><![CDATA[listings.listing.title]]></>

# Specify the full xml path for the results description.
<1PaidInclusionAdFeedXmlDescriptionTag><![CDATA[listings.listing.description]]></>

# Specify the full xml path for the results link.  This is the link that is
# shown as plain text, not an actual link, below the ad description.
<1PaidInclusionAdFeedXmlLinkTag><![CDATA[listings.listing.url]]></>

# Specify the full xml path for the results url.  This is the link associated
# with the title.
<1PaidInclusionAdFeedXmlUrlTag><![CDATA[listings.listing.uri]]></>

# Full link with address and parameters to retrieve an ad feed.  To specify
# parameter input: %q for query, %n for num results, %p for page number, %i
# for query ip, and %% for %.
<1PaidInclusionBackupAdFeedLink><![CDATA[]]></>

# Specify the full xml path for a result.
<1PaidInclusionBackupAdFeedXmlResultTag><![CDATA[]]></>

# Specify the full xml path for the results title.
<1PaidInclusionBackupAdFeedXmlTitleTag><![CDATA[]]></>

# Specify the full xml path for the results description.
<1PaidInclusionBackupAdFeedXmlDescriptionTag><![CDATA[]]></>

# Specify the full xml path for the results link.  This is the link that is
# shown as plain text, not an actual link, below the ad description.
<1PaidInclusionBackupAdFeedXmlLinkTag><![CDATA[]]></>

# Specify the full xml path for the results url.  This is the link associated
# with the title.
<1PaidInclusionBackupAdFeedXmlUrlTag><![CDATA[]]></>

# Specify the formatting text from the <div tag in
<1PaidInclusionFormatText><![CDATA[style=&#34;padding: 3px;text-align: left; background-color: lightyellow; border-bottom: 1px solid lightgrey;&#34;&gt;&lt;span style=&#34;font-size: smaller; color:grey; float:right;&#34;&gt;Sponsored Results&lt;/span&gt;]]></>

# Enable/Disable the skyscraper ad.
<1SkyscraperAdEnable>1</>

# Use the same feed CGI as used above for the paid inclusion.
<1SkyscraperAdFeedSameAsPaidInclusion>1</>

# Full link with address and parameters to retrieve an ad feed.  To specify
# parameter input: %q for query, %n for num results, %p for page number, %i
# for query ip, and %% for %.
<1SkyscraperAdFeedLink><![CDATA[]]></>

# Specify the full xml path for a result.
<1SkyscraperAdFeedXmlResultTag><![CDATA[]]></>

# Specify the full xml path for the results title.
<1SkyscraperAdFeedXmlTitleTag><![CDATA[]]></>

# Specify the full xml path for the results description.
<1SkyscraperAdFeedXmlDescriptionTag><![CDATA[]]></>

# Specify the full xml path for the results link.  This is the link that is
# shown as plain text, not an actual link, below the ad description.
<1SkyscraperAdFeedXmlLinkTag><![CDATA[]]></>

# Specify the full xml path for the results url.  This is the link associated
# with the title.
<1SkyscraperAdFeedXmlUrlTag><![CDATA[]]></>

# Use the same feed CGI as used above for the backup paid inclusion.
<1SkyscraperBackupAdFeedSameAsPaidInclusion>0</>

# Full link with address and parameters to retrieve an ad feed.  To specify
# parameter input: %q for query, %n for num results, %p for page number, %i
# for query ip, and %% for %.
<1SkyscraperBackupAdFeedLink><![CDATA[]]></>

# Specify the full xml path for a result.
<1SkyscraperBackupAdFeedXmlResultTag><![CDATA[]]></>

# Specify the full xml path for the results title.
<1SkyscraperBackupAdFeedXmlTitleTag><![CDATA[]]></>

# Specify the full xml path for the results description.
<1SkyscraperBackupAdFeedXmlDescriptionTag><![CDATA[]]></>

# Specify the full xml path for the results link.  This is the link that is
# shown as plain text, not an actual link, below the ad description.
<1SkyscraperBackupAdFeedXmlLinkTag><![CDATA[]]></>

# Specify the full xml path for the results url.  This is the link associated
# with the title.
<1SkyscraperBackupAdFeedXmlUrlTag><![CDATA[]]></>

# Specify the formatting text from the <div tag in
<1SkyscraperFormatText><![CDATA[style=&#34;height: 100%; padding: 3px; text-align: left; border-left: 1px solid lightgrey;&#34;&gt;&lt;span style=&#34;font-size: smaller; color:grey;&#34;&gt;&lt;center&gt;Sponsored Results&lt;/center&gt;&lt;/span&gt;&lt;br&gt;&lt;br&gt;]]></>

# Add relevancy score of term to date after multiplying it by this float. Only
# used when sorting by date. Acts as a slider bar between sorting by relevancy
# and date. Use 0.0 to sort strictly by date then docid. (but sort by score
# next single term queries)
<sortByDateWeight>0.000</>
